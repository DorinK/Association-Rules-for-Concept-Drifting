{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3afbb56-ede7-4cf5-b025-cd39c9f7f7cb",
   "metadata": {},
   "source": [
    "# Init\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af74de10-a0c5-4b4b-b97d-5968d42b4824",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install necessary requirements\n",
    "%pip install -r ../requirements.txt\n",
    "\n",
    "# Change working directory to root\n",
    "import os\n",
    "if os.getcwd().endswith(\"notebooks\"):\n",
    "    %cd ..\n",
    "    print(os.getcwd())\n",
    "\n",
    "# Automatically reload changes in code\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7caa03e-fead-4ea4-8532-978b947bcb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from association_finder.concept_drifts_finder import ConceptDriftsFinder\n",
    "from association_finder.models import Transaction, ConceptDriftResult\n",
    "logging.basicConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4c2f29-d796-4c19-b1b6-68bb59f76ecc",
   "metadata": {},
   "source": [
    "# Read and parse file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dbb0c5-cb08-45a2-b8a2-281e7f52c6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Dict, Optional\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "dataset_path = \"datasets/houseprices/train.csv\"\n",
    "df = pd.read_csv(train_dataset_path, index_col='Id')\n",
    "df_train, df_val = train_test_split(df, test_size=0.3)\n",
    "\n",
    "@dataclass\n",
    "class TrainParams:\n",
    "    na_columns_mean: Dict[int, float]\n",
    "    numerical_columns_cut: Dict[str, List[float]]\n",
    "    \n",
    "        \n",
    "def preprocess(df, train_params: Optional[TrainParams] = None):\n",
    "    # Defining numeric and categorical columns\n",
    "    numeric_columns = df.dtypes[(df.dtypes == \"float64\") | (df.dtypes == \"int64\")].index.tolist()\n",
    "    very_numerical = [nc for nc in numeric_columns if df[nc].nunique() > 20]\n",
    "    categorical_columns = [c for c in df.columns if c not in numeric_columns]\n",
    "    ordinals = list(set(numeric_columns) - set(very_numerical))\n",
    "\n",
    "    # Filling Null Values with the column's mean\n",
    "    na_columns = df[very_numerical].isna().sum()\n",
    "    na_columns = na_columns[na_columns > 0]\n",
    "\n",
    "    na_columns_mean = {}\n",
    "        \n",
    "    for nc in na_columns.index:\n",
    "        if train_params is None:\n",
    "            column_mean = df[nc].mean()\n",
    "\n",
    "            # Save mean\n",
    "            na_columns_mean[nc] = column_mean\n",
    "        else:\n",
    "            column_mean = train_params.na_columns_mean[nc]\n",
    "\n",
    "        df[nc].fillna(column_mean, inplace=True)\n",
    "\n",
    "\n",
    "    # Dropping and filling NA values for categorical columns:\n",
    "    # drop if at least 70% are NA:\n",
    "    nul_cols = df[categorical_columns].isna().sum() / len(df)\n",
    "    drop_us = nul_cols[nul_cols > 0.7]\n",
    "    df = df.drop(drop_us.index, axis=1)\n",
    "\n",
    "    # Fill with a new 'na' category:\n",
    "    categorical_columns = list(set(categorical_columns) - set(drop_us.index))\n",
    "    df[categorical_columns] = df[categorical_columns].fillna('na')\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # Bin numerical data\n",
    "    numerical_columns_cut = {}\n",
    "    for c in very_numerical:\n",
    "        if train_params is None:\n",
    "            try:\n",
    "                # df[c] = pd.qcut(df[c], 5, labels=[\"very low\", \"low\", \"medium\", \"high\", \"very high\"])\n",
    "                df[c], bins = pd.qcut(df[c], 5, labels=[1, 2, 3, 4, 5], retbins=True)\n",
    "            except:\n",
    "                # sometimes for highly skewed data, we cannot perform qcut as most quantiles are equal\n",
    "                # df[c] = pd.cut(df[c], 5, labels=[\"very low\", \"low\", \"medium\", \"high\", \"very high\"])\n",
    "                df[c], bins = pd.cut(df[c], 5, labels=[1, 2, 3, 4, 5], retbins=True)\n",
    "\n",
    "            # Make bin edges larger (infinity and -infinity)\n",
    "            bins = np.concatenate(([-np.inf], bins[1:-1], [np.inf]))\n",
    "\n",
    "            # Save bin\n",
    "            numerical_columns_cut[c] = bins\n",
    "            \n",
    "        else:\n",
    "            # Use existing train bins\n",
    "            bins = train_params.numerical_columns_cut[c]\n",
    "            df[c] = pd.cut(df[c], labels=[1, 2, 3, 4, 5], bins=bins)\n",
    "        \n",
    "    return df, TrainParams(na_columns_mean, numerical_columns_cut)\n",
    "\n",
    "df_train_prep, train_params = preprocess(df_train)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0be935-2ff0-4a66-94a1-10cadd736129",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08e2951-527c-404d-a66b-c683ca6aa6d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from typing import Tuple\n",
    "\n",
    "def split_X_y(df_prep: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    X_columns = list(set(good_columns) - {target_column})\n",
    "    X = df_prep[X_columns]\n",
    "\n",
    "    one_hot_feature = 'BldgType'\n",
    "    X = pd.concat([X, pd.get_dummies(X[one_hot_feature], prefix=one_hot_feature)], axis=1)\n",
    "    X = X.drop(columns=[one_hot_feature])\n",
    "\n",
    "    y = df_prep[target_column]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = split_X_y(df_train_prep)\n",
    "X_val, y_val = split_X_y(preprocess(df_val, train_params)[0])\n",
    "\n",
    "clf = LogisticRegression(random_state=0).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f952dcfe-cf51-4e1d-85b3-77008e96c376",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Train accuracy: {clf.score(X_train, y_train)}\")\n",
    "print(f\"Validation accuracy: {clf.score(X_val, y_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a137dbcb-658c-4d1a-8493-43de2b6e743c",
   "metadata": {},
   "source": [
    "# Find rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d387654-987f-4b56-8e10-c68196ad1eaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Focusing on prominent columns:\n",
    "good_columns = ['OverallQual', 'YearBuilt', 'YearRemodAdd', 'OverallCond', 'BldgType', 'LotArea',\n",
    "                'GrLivArea', 'FullBath', 'BedroomAbvGr', 'LotFrontage', 'TotalBsmtSF', 'SalePrice']\n",
    "# TODO: Check where Amit took the good columns list from\n",
    "\n",
    "target_column = \"SalePrice\"\n",
    "\n",
    "# We need to convert our dataframe to a list of transactions\n",
    "records = df[good_columns].to_dict(orient='records')\n",
    "transactions = []\n",
    "for r in records:\n",
    "    transactions.append(Transaction({k: v for k, v in r.items()}))\n",
    "\n",
    "potential_concept_columns = good_columns.copy()\n",
    "potential_concept_columns.remove(target_column)\n",
    "all_concepts = []\n",
    "for concept_column in potential_concept_columns[:4]:\n",
    "    try:\n",
    "        print()\n",
    "        print(f\"Starting concept column '{concept_column}'\")\n",
    "        \n",
    "        # Run the ConceptDriftsFinder\n",
    "        concepts: List[ConceptDriftResult] = ConceptDriftsFinder().find_concept_drifts(transactions, concept_column,\n",
    "                                                                                       target_column, min_confidence=0.4,\n",
    "                                                                                       min_support=0.4, diff_threshold=0.1)\n",
    "\n",
    "        # Convert to dataframe\n",
    "        all_concepts.extend(concepts)\n",
    "        concepts_df = pd.DataFrame([x.to_dict() for x in concepts])\n",
    "\n",
    "        # Print dataframe\n",
    "        pd.set_option(\"display.max_columns\", 20)\n",
    "        print(concepts_df.head())\n",
    "    except:\n",
    "        print(f\"Failed concept column '{concept_column}'\")\n",
    "        # logging.exception(f\"Failed concept column {concept_column}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0909d2-541a-4970-bd33-e475df5705cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame([x.to_dict() for x in all_concepts])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064defeb-3848-4d30-ad26-dddc0fa26563",
   "metadata": {},
   "source": [
    "# Build model using rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b32692a-e1f3-4829-91d9-0cc1f5396aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rules[X_train_rules['OverallQual'] >= 8.2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c65647-d41e-4d1b-be20-70d75c312782",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_X(X):\n",
    "    X.loc[(X['OverallQual'] >= 4.6) & (X['OverallQual'] < 8.2), 'BldgType_1Fam'] = 0\n",
    "    X.loc[(X['OverallQual'] >= 4.6) & (X['OverallQual'] < 8.2) & (X['FullBath'] == 1), 'FullBath'] = 0\n",
    "\n",
    "    X.loc[X['OverallQual'] >= 8.2, 'BldgType_1Fam'] = X.loc[X['OverallQual'] >= 8.2, 'BldgType_1Fam'] * 2\n",
    "    X.loc[(X['OverallQual'] >= 8.2) & (X['FullBath'] == 2), 'FullBath'] = X.loc[(X['OverallQual'] >= 8.2) & (X['FullBath'] == 2), 'FullBath'] * 2\n",
    "    \n",
    "    return X\n",
    "\n",
    "X_train_rules = X_train.copy()\n",
    "X_val_rules = X_val.copy()\n",
    "\n",
    "X_train_rules = modify_X(X_train_rules)\n",
    "X_val_rules = modify_X(X_val_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899319bf-5b89-4021-a1a1-7263259c2914",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rules = LogisticRegression(random_state=0).fit(X_train_rules, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ec9a9b-9c2f-4326-8c43-c1cb4c94777b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train accuracy: {clf_rules.score(X_train_rules, y_train)}\")\n",
    "print(f\"Validation accuracy: {clf_rules.score(X_val_rules, y_val)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eran_nlp_38",
   "language": "python",
   "name": "eran_nlp_38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
